# Example System Configuration for RHEL Lightspeed Evaluation
# Copy this to system.yaml and configure for your environment

# LLM Configuration
llm:
  provider: vertex  # Options: openai, azure, watsonx, anthropic, gemini, ollama, etc.
  model: gemini-2.0-flash
  temperature: 0.0
  max_tokens: 500
  timeout: 30
  num_retries: 3
  cache_dir: ".caches/llm_cache"   
  cache_enabled: true

# Embedding Configuration
embedding:
  provider: "openai"  # Options: openai, huggingface
  model: "text-embedding-ada-002"
  cache_enabled: true

api:
  enabled: false

# GEval Metrics Configuration
geval:
  enabled: true
  registry_path: "config/registry/geval_metrics.example.yaml"

# Output Configuration
output:
  output_dir: "./eval_output"
  base_filename: "evaluation"
  enabled_outputs:          # Enable specific output types
    - csv                   # Detailed results CSV
    - json                  # Summary JSON with statistics
    - txt                   # Human-readable summary

  # CSV columns to include
  csv_columns:
    - "conversation_group_id"
    - "turn_id"
    - "metric_identifier"
    - "score"
    - "threshold"
    - "result"
    - "reason"
    - "query"
    - "response"
    - "execution_time"


# Logging Configuration
logging:
  # Source code logging level
  source_level: INFO          # DEBUG, INFO, WARNING, ERROR, CRITICAL

  # Package logging level (imported libraries)
  package_level: ERROR

  # Log format and display options
  log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  show_timestamps: true

# Core Configuration
core:
  max_threads: 4  # Number of parallel conversation threads

# Default metrics metadata
metrics_metadata:
  # Turn-level metrics metadata
  turn_level:
    # Custom metrics
    "custom:answer_correctness":
      threshold: 0.5
      description: "Correctness vs expected answer using custom LLM evaluation"

    "geval:technical_accuracy":
        threshold: 0.8
        description: "Technical accuracy of RHEL commands"

    # Script-based metrics
    "script:action_eval":
      description: "Script-based evaluation for infrastructure/environment validation"

# Visualization settings
visualization:
  figsize: [12, 8]            # Graph size (width, height)
  dpi: 300                    # Image resolution

  # Graph types to generate
  enabled_graphs:
    - "pass_rates"            # Pass rate bar chart
    - "score_distribution"    # Score distribution box plot
    - "conversation_heatmap"  # Heatmap of conversation performance
    - "status_breakdown"      # Pie chart for pass/fail/error breakdown
